write.xls(dtt, "dtt")
library(xlsReadWrite)
install.package(xlsReadWrite)
install.package("xlsReadWrite"")
install.package("xlsReadWrite")
install.package("xlsReadWrite")
install.packages("xlsReadWrite")
library(WriteXLS)
install.packages("WriteXLS")
write.xls(dtt, "dtt")
WriteXLS("dtt", "dtt.xls")
library("WriteXLS", lib.loc="/Library/Frameworks/R.framework/Versions/3.0/Resources/library")
WriteXLS("dtt", "dtt.xls")
dtt[, list(Mean = rowMeans(.SD)), by = influenceDiagramName]
dtt[, list(Mean = rowMeans(inferenceTimeTaken)), by = influenceDiagramName]
View(dtt)
dtt <- subset(vInferenceTimeTaken, select = -c(inferenceIteration,inferenceDate,inferenceTime))
dtt <- data.table(dtt)
dtt[, list(Mean = rowMeans(inferenceTimeTaken)), by = influenceDiagramName]
View(dtt)
dtt[, list(Mean = mean(inferenceTimeTaken)), by = influenceDiagramName]
dtt[, list(Mean = mean(inferenceTimeTaken)), by = list(influenceDiagramName,inferenceAlgorithmID)]
dtt[, list(Mean = mean(inferenceTimeTaken,trim=0.1)), by = list(influenceDiagramName,inferenceAlgorithmID)]
dtt[, list(Mean = mean(inferenceTimeTaken,trim=0.1),influenceDiagramChanceDecisionNodesRatio), by = list(influenceDiagramName,inferenceAlgorithmID)]
dtt[, list(Mean = mean(inferenceTimeTaken,trim=0.1)), by = list(influenceDiagramName,inferenceAlgorithmID,dtt[, list(Mean = mean(inferenceTimeTaken,trim=0.1),influenceDiagramChanceDecisionNodesRatio), by = list(influenceDiagramName,inferenceAlgorithmID)])]
dtt[, list(Mean = mean(inferenceTimeTaken,trim=0.1)), by = list(influenceDiagramName,inferenceAlgorithmID,influenceDiagramChanceDecisionNodesRatio)]
dtt2 <- dtt[, list(Mean = mean(inferenceTimeTaken,trim=0.1)), by = list(influenceDiagramName,inferenceAlgorithmID,influenceDiagramChanceDecisionNodesRatio)]
qplot(influenceDiagramChanceDecisionNodesRatio,Mean,data=dtt2)
ggplot(dtt2, aes(influenceDiagramChanceDecisionNodesRatio, Mean))+
geom_point(aes(color = factor(inferenceAlgorithmID)))+
geom_line(
)
png(filename="firstPlot.png")
plot(firstPlot)
p <- ggplot(dtt2, aes(influenceDiagramChanceDecisionNodesRatio, Mean))+
geom_point(aes(color = factor(inferenceAlgorithmID)))+
geom_line()
plot(p)
dev.off()
png("image.png", width = 800, height = 600)
plot(p)
dev.off()
png("image.png", width = 1024, height = 768)
plot(p)
dev.off()
p <- ggplot(dtt2, aes(influenceDiagramChanceDecisionNodesRatio, Mean))+
geom_point(aes(color = factor(inferenceAlgorithmID)))
print(p)
p <- ggplot(dtt2, aes(influenceDiagramChanceDecisionNodesRatio, Mean))+
geom_point(aes(color = factor(inferenceAlgorithm)))
print(p)
p <- ggplot(dtt2, aes(influenceDiagramChanceDecisionNodesRatio, Mean))+
geom_point(aes(color = factor(inferenceAlgorithmID)))
print(p)
p <- ggplot(dtt2, aes(influenceDiagramChanceDecisionNodesRatio, Mean), shape=am, color=am)+
geom_point(aes(color = factor(inferenceAlgorithmID)))
print(p)
p <- ggplot(dtt2, aes(influenceDiagramChanceDecisionNodesRatio, Mean))+
geom_point(aes(color = factor(inferenceAlgorithmID)))
print(p)
library(lattice)
View(dtt2)
View(dtt2)
dtt2 <- dtt[, list(Mean = mean(inferenceTimeTaken,trim=0.1)), by = list(influenceDiagramName,inferenceAlgorithmID,influenceDiagramChanceDecisionNodesRatio)]
dtt[, list(Mean = mean(inferenceTimeTaken,trim=0.1)), by = list(influenceDiagramName,inferenceAlgorithmID,influenceDiagramChanceDecisionNodesRatio)]
dtt[, list(Mean = mean(inferenceTimeTaken,trim=0.1),InferenceAlgorithm,inferenceAlgorithmID,influenceDiagramChanceDecisionNodesRatio,influenceDiagramChanceAndDecisionNodes,influenceDiagramUtilityNodes), by = list(influenceDiagramName)]
dtt[, list(Mean = mean(inferenceTimeTaken,trim=0.1),InferenceAlgorithm,inferenceAlgorithmID,influenceDiagramChanceDecisionNodesRatio,influenceDiagramChanceAndDecisionNodes,influenceDiagramUtilityNodes), by = list(influenceDiagramName)]
dtt[, list(InferenceAlgorithm,inferenceAlgorithmID,influenceDiagramChanceDecisionNodesRatio,influenceDiagramChanceAndDecisionNodes,influenceDiagramUtilityNodes), by = list(influenceDiagramName)]
dtt[, list(inferenceAlgorithm,inferenceAlgorithmID,influenceDiagramChanceDecisionNodesRatio,influenceDiagramChanceAndDecisionNodes,influenceDiagramUtilityNodes), by = list(influenceDiagramName)]
dtt[, list(Mean = mean(inferenceTimeTaken,trim=0.1),inferenceAlgorithm,inferenceAlgorithmID,influenceDiagramChanceDecisionNodesRatio,influenceDiagramChanceAndDecisionNodes,influenceDiagramUtilityNodes), by = list(influenceDiagramName)]
dtt2 <- dtt[, list(Mean = mean(inferenceTimeTaken,trim=0.1),inferenceAlgorithm,inferenceAlgorithmID,influenceDiagramChanceDecisionNodesRatio,influenceDiagramChanceAndDecisionNodes,influenceDiagramUtilityNodes), by = list(influenceDiagramName)]
View(dtt2)
dtt2 <- dtt[, list(Mean = mean(inferenceTimeTaken,trim=0.1),inferenceAlgorithm,inferenceAlgorithmID,influenceDiagramChanceDecisionNodesRatio,influenceDiagramChanceAndDecisionNodes,influenceDiagramUtilityNodes), by =influenceDiagramName]
View(dtt2)
dtt2 <- dtt[, list(Mean = mean(inferenceTimeTaken,trim=0.1)), by = list(influenceDiagramName,inferenceAlgorithm,inferenceAlgorithmID,influenceDiagramChanceDecisionNodesRatio,influenceDiagramChanceAndDecisionNodes,influenceDiagramUtilityNodes)]
View(dtt2)
cloud(Mean ~ influenceDiagramChanceDecisionNodesRatio * influenceDiagramUtilityNodes, data = dtt2,
panel.aspect = 0.75)
cloud(Mean ~ influenceDiagramChanceDecisionNodesRatio * influenceDiagramUtilityNodes, data = dtt2,
panel.aspect = 0.75,color = factor(inferenceAlgorithm))
cloud(Mean ~ influenceDiagramChanceDecisionNodesRatio * influenceDiagramUtilityNodes, data = dtt2,
panel.aspect = 0.75,color = c(inferenceAlgorithm))
View(dtt2)
cloud(Mean ~ influenceDiagramChanceDecisionNodesRatio * influenceDiagramUtilityNodes, data = dtt2,
panel.aspect = 0.75,col = c(inferenceAlgorithm))
cloud(Mean ~ influenceDiagramChanceDecisionNodesRatio * influenceDiagramUtilityNodes, data = dtt2,
panel.aspect = 0.75,col = "black")
cloud(Mean ~ influenceDiagramChanceDecisionNodesRatio * influenceDiagramUtilityNodes, data = dtt2,
panel.aspect = 0.75,groups=inferenceAlgorithm)
cloud(Mean ~ influenceDiagramChanceDecisionNodesRatio * influenceDiagramUtilityNodes, data = dtt2,
panel.aspect = 0.75,groups=inferenceAlgorithm,labz=inference time taken mean trimmed)
cloud(Mean ~ influenceDiagramChanceDecisionNodesRatio * influenceDiagramUtilityNodes, data = dtt2,
panel.aspect = 0.75,groups=inferenceAlgorithm,labz=inference time taken mean trimmed)
cloud(Mean ~ influenceDiagramChanceDecisionNodesRatio * influenceDiagramUtilityNodes, data = dtt2,
panel.aspect = 0.75,groups=inferenceAlgorithm,labz="inference time taken mean trimmed")
cloud(Mean ~ influenceDiagramChanceDecisionNodesRatio * influenceDiagramUtilityNodes, data = dtt2,
panel.aspect = 0.75,groups=inferenceAlgorithm,labz="inference time taken mean trimmed")
cloud(Mean ~ influenceDiagramChanceDecisionNodesRatio * influenceDiagramUtilityNodes, data = dtt2,
panel.aspect = 0.75,groups=inferenceAlgorithm,zlab="inference time taken mean trimmed")
cloud(Mean ~ influenceDiagramChanceDecisionNodesRatio * influenceDiagramUtilityNodes, data = dtt2,
panel.aspect = 0.75,groups=inferenceAlgorithm,zlab="inference time mean trimmed")
cloud(Mean ~ influenceDiagramChanceDecisionNodesRatio * influenceDiagramUtilityNodes, data = dtt2,
panel.aspect = 0.75,groups=inferenceAlgorithm,zlab="inference time mean trimmed",xlab="ratio")
cloud(Mean ~ influenceDiagramChanceDecisionNodesRatio * influenceDiagramUtilityNodes, data = dtt2,
panel.aspect = 0.75,groups=inferenceAlgorithm,zlab="inference time mean trimmed",xlab="ratio",ylab="utility nodes")
cloud(Mean ~ influenceDiagramChanceDecisionNodesRatio * influenceDiagramUtilityNodes, data = dtt2,
panel.aspect = 0.75,groups=inferenceAlgorithm,zlab="inference time mean trimmed",xlab="ratio",ylab="utility nodes"distance=5)
cloud(Mean ~ influenceDiagramChanceDecisionNodesRatio * influenceDiagramUtilityNodes, data = dtt2,
panel.aspect = 0.75,groups=inferenceAlgorithm,zlab="inference time mean trimmed",xlab="ratio",ylab="utility nodes",distance=5)
cloud(Mean ~ influenceDiagramChanceDecisionNodesRatio * influenceDiagramUtilityNodes, data = dtt2,
panel.aspect = 0.75,groups=inferenceAlgorithm,zlab="inference time mean trimmed",xlab="ratio",ylab="utility nodes",distance=100)
cloud(Mean ~ influenceDiagramChanceDecisionNodesRatio * influenceDiagramUtilityNodes, data = dtt2,
panel.aspect = 0.75,groups=inferenceAlgorithm,zlab="inference time mean trimmed",xlab="ratio",ylab="utility nodes",distance=1000)
cloud(Mean ~ influenceDiagramChanceDecisionNodesRatio * influenceDiagramUtilityNodes, data = dtt2,
panel.aspect = 0.75,groups=inferenceAlgorithm,zlab="inference time mean trimmed",xlab="ratio",ylab="utility nodes",distance=10000)
cloud(Mean ~ influenceDiagramChanceDecisionNodesRatio * influenceDiagramUtilityNodes, data = dtt2,
panel.aspect = 0.75,groups=inferenceAlgorithm,zlab="inference time mean trimmed",xlab="ratio",ylab="utility nodes",distance=50)
cloud(Mean ~ influenceDiagramChanceDecisionNodesRatio * influenceDiagramUtilityNodes, data = dtt2,
panel.aspect = 0.75,groups=inferenceAlgorithm,zlab="inference time mean trimmed",xlab="ratio",ylab="utility nodes")
ggplot(dtt2, aes(influenceDiagramChanceDecisionNodesRatio, Mean))+
geom_point(aes(color = factor(inferenceAlgorithm)))+
geom_line()
ggplot(dtt2, aes(influenceDiagramChanceDecisionNodesRatio, Mean))+
geom_point(aes(color = factor(inferenceAlgorithm)))
ggplot(dtt2, aes(influenceDiagramChanceDecisionNodesRatio, Mean))+
geom_point(aes(color = factor(influenceDiagramName)))
ggplot(dtt2, aes(influenceDiagramChanceDecisionNodesRatio, Mean))+
geom_point(aes(color = factor(inferenceAlgorithm)))
ggplot(dtt2, aes(influenceDiagramChanceDecisionNodesRatio, Mean))+
geom_point(aes(color = factor(inferenceAlgorithm)))+geom_density()
ggplot(dtt2, aes(influenceDiagramChanceDecisionNodesRatio, Mean))+
geom_point(aes(color = factor(inferenceAlgorithm)))+
geom_density()
ggplot(dtt2, aes(influenceDiagramChanceDecisionNodesRatio, Mean))+
geom_point(aes(color = factor(inferenceAlgorithm)))+geom_density()+coord_cartesian(ylim=c(0, 0.1))
ggplot(dtt2, aes(influenceDiagramChanceDecisionNodesRatio, Mean))+
geom_point(aes(color = factor(inferenceAlgorithm)))+
+coord_cartesian(ylim=c(0, 0.1))
ggplot(dtt2, aes(influenceDiagramChanceDecisionNodesRatio, Mean))+
geom_point(aes(color = factor(inferenceAlgorithm)))+coord_cartesian(ylim=c(0, 0.1))
ggplot(dtt2, aes(influenceDiagramChanceDecisionNodesRatio, Mean))+
geom_point(aes(color = factor(inferenceAlgorithm)))+coord_cartesian(ylim=c(0, 1e10))
ggplot(dtt2, aes(influenceDiagramChanceDecisionNodesRatio, Mean))+
geom_point(aes(color = factor(inferenceAlgorithm)))+coord_cartesian(ylim=c(0, 1e1))
ggplot(dtt2, aes(influenceDiagramChanceDecisionNodesRatio, Mean))+
geom_point(aes(color = factor(inferenceAlgorithm)))+coord_cartesian(ylim=c(0, 1e2))
ggplot(dtt2, aes(influenceDiagramChanceDecisionNodesRatio, Mean))+
geom_point(aes(color = factor(inferenceAlgorithm)))+coord_cartesian(ylim=c(0, 1e5))
ggplot(dtt2, aes(influenceDiagramChanceDecisionNodesRatio, Mean))+
geom_point(aes(color = factor(inferenceAlgorithm)))+coord_cartesian(ylim=c(0, 1e8))
ggplot(dtt2, aes(influenceDiagramChanceDecisionNodesRatio, Mean))+
geom_point(aes(color = factor(inferenceAlgorithm)))+coord_cartesian(ylim=c(0, 2.5e7))
subset(dtt2, influenceDiagramChanceAndDecisionNodes=40)
subset(dtt2, influenceDiagramChanceAndDecisionNodes==40)
ggplot(subset(dtt2, influenceDiagramChanceAndDecisionNodes==40), aes(influenceDiagramChanceDecisionNodesRatio, Mean))+
geom_point(aes(color = factor(inferenceAlgorithm))))
ggplot(subset(dtt2, influenceDiagramChanceAndDecisionNodes==40), aes(influenceDiagramChanceDecisionNodesRatio, Mean))+geom_point(aes(color = factor(inferenceAlgorithm)))
ggplot(subset(dtt2, influenceDiagramChanceAndDecisionNodes==50), aes(influenceDiagramChanceDecisionNodesRatio, Mean))+geom_point(aes(color = factor(inferenceAlgorithm)))
ggplot(subset(dtt2, influenceDiagramChanceAndDecisionNodes==50), aes(influenceDiagramUtilityNodes, Mean))+geom_point(aes(color = factor(inferenceAlgorithm)))
ggplot(subset(dtt2, influenceDiagramChanceAndDecisionNodes==40), aes(influenceDiagramUtilityNodes, Mean))+geom_point(aes(color = factor(inferenceAlgorithm)))
# load the necessary libraries
library(RMySQL)
# connect to the database
con <- dbConnect(MySQL(),user="artasom",password="artasom",dbname="artasomThesisDBTest",host="localhost")
# read the data of the time taken by the algorithms
vInferenceTimeTaken = dbReadTable(con, 'vInferenceTimeTaken')
# read the data of the memory used by the algorithms
vInferenceMemoryUsed = dbReadTable(con, 'vInferenceMemoryUsed')
# check the information read
head(vInferenceTimeTaken,4)
head(vInferenceMemoryUsed,4)
# disconnect from database
dbDisconnect(con)
# unload the libraries
detach("package:RMySQL")
vInferenceTimeTakenVE <- subset(vInferenceTimeTaken,inferenceAlgorithmID=1)
vInferenceTimeTakenAR <- subset(vInferenceTimeTaken,inferenceAlgorithmID=2)
library(data.table)
tbInferenceTimeTakenVE <- data.table(vInferenceTimeTakenVE)
tbInferenceTimeTakenAR <- data.table(vInferenceTimeTakenAR)
View(vInferenceMemoryUsed)
View(vInferenceMemoryUsed)
detach(vInferenceTimeTakenVE)
rm(vInferenceTimeTakenVE)
rm(vInferenceTimeTakenAR)
rm(dt)
rm(dtt)
rm(dtt2)
tbInferenceTimeTakenAR[, list(Mean = mean(inferenceTimeTaken)), by = list(influenceDiagramName,inferenceAlgorithmID)]
tbInferenceTimeTakenAR[, list(Max = max(inferenceTimeTaken)), by = list(influenceDiagramName,inferenceAlgorithmID)]
tbInferenceTimeTakenVE[, list(Max = max(inferenceTimeTaken)), by = list(influenceDiagramName,inferenceAlgorithmID)]
tbInferenceTimeTakenVE[, list(Max = max(inferenceTimeTaken)), by = list(influenceDiagramName)]
tbInferenceTimeTakenVE[, list(Mean = mean(inferenceTimeTaken),MeanTrim01 = mean(inferenceTimeTaken,trim=0.1),Median = median(inferenceTimeTaken),Min = min(inferenceTimeTaken),Max = max(inferenceTimeTaken),SD = sd(inferenceTimeTaken),Percentile5 = quantile(inferenceTimeTaken,.05),Percentile95 = quantile(inferenceTimeTaken,.95)), by = list(influenceDiagramName,inferenceAlgorithm,influenceDiagramChanceDecisionNodesRatio,influenceDiagramChanceAndDecisionNodes,influenceDiagramUtilityNodes)]
summaryVE <- tbInferenceTimeTakenVE[, list(Mean = mean(inferenceTimeTaken),MeanTrim01 = mean(inferenceTimeTaken,trim=0.1),Median = median(inferenceTimeTaken),Min = min(inferenceTimeTaken),Max = max(inferenceTimeTaken),SD = sd(inferenceTimeTaken),Percentile5 = quantile(inferenceTimeTaken,.05),Percentile95 = quantile(inferenceTimeTaken,.95)), by = list(influenceDiagramName,inferenceAlgorithm,influenceDiagramChanceDecisionNodesRatio,influenceDiagramChanceAndDecisionNodes,influenceDiagramUtilityNodes)]
summaryAR <- tbInferenceTimeTakenAR[, list(Mean = mean(inferenceTimeTaken),MeanTrim01 = mean(inferenceTimeTaken,trim=0.1),Median = median(inferenceTimeTaken),Min = min(inferenceTimeTaken),Max = max(inferenceTimeTaken),SD = sd(inferenceTimeTaken),Percentile5 = quantile(inferenceTimeTaken,.05),Percentile95 = quantile(inferenceTimeTaken,.95)), by = list(influenceDiagramName,inferenceAlgorithm,influenceDiagramChanceDecisionNodesRatio,influenceDiagramChanceAndDecisionNodes,influenceDiagramUtilityNodes)]
View(tbInferenceTimeTakenVE)
View(tbInferenceTimeTakenVE)
vInferenceTimeTakenVE <- subset(vInferenceTimeTaken,inferenceAlgorithmID=1)
vInferenceTimeTakenVE <- subset(vInferenceTimeTaken,inferenceAlgorithmID==1)
vInferenceTimeTakenAR <- subset(vInferenceTimeTaken,inferenceAlgorithmID==2)
summaryVE <- data.table(subset(vInferenceTimeTaken,inferenceAlgorithmID==1))[, list(Mean = mean(inferenceTimeTaken),MeanTrim01 = mean(inferenceTimeTaken,trim=0.1),Median = median(inferenceTimeTaken),Min = min(inferenceTimeTaken),Max = max(inferenceTimeTaken),SD = sd(inferenceTimeTaken),Percentile5 = quantile(inferenceTimeTaken,.05),Percentile95 = quantile(inferenceTimeTaken,.95)), by = list(influenceDiagramName,inferenceAlgorithm,influenceDiagramChanceDecisionNodesRatio,influenceDiagramChanceAndDecisionNodes,influenceDiagramUtilityNodes)]
summaryAR <- data.table(subset(vInferenceTimeTaken,inferenceAlgorithmID==2))[, list(Mean = mean(inferenceTimeTaken),MeanTrim01 = mean(inferenceTimeTaken,trim=0.1),Median = median(inferenceTimeTaken),Min = min(inferenceTimeTaken),Max = max(inferenceTimeTaken),SD = sd(inferenceTimeTaken),Percentile5 = quantile(inferenceTimeTaken,.05),Percentile95 = quantile(inferenceTimeTaken,.95)), by = list(influenceDiagramName,inferenceAlgorithm,influenceDiagramChanceDecisionNodesRatio,influenceDiagramChanceAndDecisionNodes,influenceDiagramUtilityNodes)]
View(summaryAR)
summaryAR$Mean / summaryVE$Mean
View(summaryAR)
View(summaryVE)
compareSummary <- data.table(summaryAR[,influenceDiagramName],summaryAR[,influenceDiagramChanceDecisionNodesRatio], )
data.table(summaryAR[,influenceDiagramName],summaryAR[,influenceDiagramChanceDecisionNodesRatio])
subset(vInferenceTimeTaken,inferenceAlgorithmID==2)
data.table(subset(vInferenceTimeTaken,inferenceAlgorithmID==2))[, list(Mean = mean(inferenceTimeTaken),MeanTrim01 = mean(inferenceTimeTaken,trim=0.1),Median = median(inferenceTimeTaken),Min = min(inferenceTimeTaken),Max = max(inferenceTimeTaken),SD = sd(inferenceTimeTaken),Percentile5 = quantile(inferenceTimeTaken,.05),Percentile95 = quantile(inferenceTimeTaken,.95)), by = list(inferenceAlgorithm,influenceDiagramChanceDecisionNodesRatio,influenceDiagramChanceAndDecisionNodes,influenceDiagramUtilityNodes)]
test <- data.table(subset(vInferenceTimeTaken,inferenceAlgorithmID==2))[, list(Mean = mean(inferenceTimeTaken),MeanTrim01 = mean(inferenceTimeTaken,trim=0.1),Median = median(inferenceTimeTaken),Min = min(inferenceTimeTaken),Max = max(inferenceTimeTaken),SD = sd(inferenceTimeTaken),Percentile5 = quantile(inferenceTimeTaken,.05),Percentile95 = quantile(inferenceTimeTaken,.95)), by = list(inferenceAlgorithm,influenceDiagramChanceDecisionNodesRatio,influenceDiagramChanceAndDecisionNodes,influenceDiagramUtilityNodes)]
View(test)
summaryAR <- data.table(subset(vInferenceTimeTaken,inferenceAlgorithmID==2))[, list(Mean = mean(inferenceTimeTaken),MeanTrim01 = mean(inferenceTimeTaken,trim=0.1),Median = median(inferenceTimeTaken),Min = min(inferenceTimeTaken),Max = max(inferenceTimeTaken),SD = sd(inferenceTimeTaken),Percentile5 = quantile(inferenceTimeTaken,.05),Percentile95 = quantile(inferenceTimeTaken,.95)), by = list(inferenceAlgorithm,influenceDiagramChanceDecisionNodesRatio,influenceDiagramChanceAndDecisionNodes,influenceDiagramUtilityNodes)]
summaryVE <- data.table(subset(vInferenceTimeTaken,inferenceAlgorithmID==1))[, list(Mean = mean(inferenceTimeTaken),MeanTrim01 = mean(inferenceTimeTaken,trim=0.1),Median = median(inferenceTimeTaken),Min = min(inferenceTimeTaken),Max = max(inferenceTimeTaken),SD = sd(inferenceTimeTaken),Percentile5 = quantile(inferenceTimeTaken,.05),Percentile95 = quantile(inferenceTimeTaken,.95)), by = list(inferenceAlgorithm,influenceDiagramChanceDecisionNodesRatio,influenceDiagramChanceAndDecisionNodes,influenceDiagramUtilityNodes)]
data.table(summaryAR[,influenceDiagramName],summaryAR[,influenceDiagramChanceDecisionNodesRatio],summaryAR[,influenceDiagramChanceAndDecisionNodes],summaryAR[,influenceDiagramUtilityNodes],
summaryAR[,Mean]/summaryVE[,Mean],summaryAR[,MeanTrim01]/summaryVE[,MeanTrim01],
summaryAR[,Median]/summaryVE[,Median],
summaryAR[,Min]/summaryVE[,Min],
summaryAR[,Max]/summaryVE[,Max],
summaryAR[,Percentile5]/summaryVE[,Percentile5],
summaryAR[,Percentile95]/summaryVE[,Percentile95]
)
data.table(summaryAR[,influenceDiagramChanceDecisionNodesRatio],summaryAR[,influenceDiagramChanceAndDecisionNodes],summaryAR[,influenceDiagramUtilityNodes],
summaryAR[,Mean]/summaryVE[,Mean],summaryAR[,MeanTrim01]/summaryVE[,MeanTrim01],
summaryAR[,Median]/summaryVE[,Median],
summaryAR[,Min]/summaryVE[,Min],
summaryAR[,Max]/summaryVE[,Max],
summaryAR[,Percentile5]/summaryVE[,Percentile5],
summaryAR[,Percentile95]/summaryVE[,Percentile95]
)
data.table(ChanceDecisionRatio = summaryAR[,influenceDiagramChanceDecisionNodesRatio], ChanceDecisionNodeNumber = summaryAR[,influenceDiagramChanceAndDecisionNodes], UtilityNodeNumber = summaryAR[,influenceDiagramUtilityNodes],
Mean = summaryAR[,Mean]/summaryVE[,Mean], MeanTrim01 = summaryAR[,MeanTrim01]/summaryVE[,MeanTrim01],
Median = summaryAR[,Median]/summaryVE[,Median],
Min = summaryAR[,Min]/summaryVE[,Min],
Max = summaryAR[,Max]/summaryVE[,Max],
Percentile5 = summaryAR[,Percentile5]/summaryVE[,Percentile5],
Percentile95 = summaryAR[,Percentile95]/summaryVE[,Percentile95]
)
summaryComparison <- data.table(ChanceDecisionRatio = summaryAR[,influenceDiagramChanceDecisionNodesRatio], ChanceDecisionNodeNumber = summaryAR[,influenceDiagramChanceAndDecisionNodes], UtilityNodeNumber = summaryAR[,influenceDiagramUtilityNodes],
Mean = summaryAR[,Mean]/summaryVE[,Mean], MeanTrim01 = summaryAR[,MeanTrim01]/summaryVE[,MeanTrim01],
Median = summaryAR[,Median]/summaryVE[,Median],
Min = summaryAR[,Min]/summaryVE[,Min],
Max = summaryAR[,Max]/summaryVE[,Max],
Percentile5 = summaryAR[,Percentile5]/summaryVE[,Percentile5],
Percentile95 = summaryAR[,Percentile95]/summaryVE[,Percentile95]
View(test)
summaryComparison <- data.table(ChanceDecisionRatio = summaryAR[,influenceDiagramChanceDecisionNodesRatio], ChanceDecisionNodeNumber = summaryAR[,influenceDiagramChanceAndDecisionNodes], UtilityNodeNumber = summaryAR[,influenceDiagramUtilityNodes],
Mean = summaryAR[,Mean]/summaryVE[,Mean], MeanTrim01 = summaryAR[,MeanTrim01]/summaryVE[,MeanTrim01],
Median = summaryAR[,Median]/summaryVE[,Median],
Min = summaryAR[,Min]/summaryVE[,Min],
Max = summaryAR[,Max]/summaryVE[,Max],
Percentile5 = summaryAR[,Percentile5]/summaryVE[,Percentile5],
Percentile95 = summaryAR[,Percentile95]/summaryVE[,Percentile95])
View(summaryComparison)
Write.table(summaryComparison)
write.table(summaryComparison,"summaryComparison.xls")
write.xls(summaryComparison, "summaryComparison.xls",
colNames = TRUE,
sheet = 1,
from = 1,
rowNames = NA,
naStrings = "")
write.xls(summaryComparison, summaryComparison.xls,
colNames = TRUE,
sheet = 1,
from = 1,
rowNames = NA,
naStrings = "")
write.xls(summaryComparison, file="summaryComparison.xls",
colNames = TRUE,
sheet = 1,
from = 1,
rowNames = NA,
naStrings = "")
install.packages("WriteXLS")
WriteXLS("summaryComparison", "summaryComparison.xls")
library("WriteXLS", lib.loc="/Library/Frameworks/R.framework/Versions/3.0/Resources/library")
WriteXLS("summaryComparison", "summaryComparison.xls")
install.packages("MASS")
install.packages('knitr', dependencies = TRUE)
library("knitr", lib.loc="/Library/Frameworks/R.framework/Versions/3.1/Resources/library")
install.packages("ProjectTemplate")
install.packages("RMySQL")
hist(rowsum(activityWithOutNAs[['steps']], activityWithOutNAs[['date']]), breaks=20,col = "blue1", main = 'Total number of steps taken each day')
hist(rowsum(activityWithOutNAs[['steps']], activityWithOutNAs[['date']]),
xaxt='n',
breaks=20,col = "grey", main = 'Total number of steps taken each day')
# Data is loaded into a data frame called activity
activity <- read.csv("data/activity.csv")
# The NAs values are removed from activity
activityWithOutNAs <- na.omit(activity)
activity <- read.csv("data/activity.csv")
graphics::plot.default(
x = activityWithOutNAs.df$interval,
y = activityWithOutNAs.df$steps,
type = "l",
xlab = "5-minute interval",
ylab = "Steps",
main = "S&P 500 (graphics::plot.default)"
)
graphics::plot.default(
x = activityWithOutNAs$interval,
y = activityWithOutNAs$steps,
type = "l",
xlab = "5-minute interval",
ylab = "Steps",
main = "S&P 500 (graphics::plot.default)"
)
activity <- read.csv("data/activity.csv")
ggplot(activityWithOutNAs) + geom_line(aes(interval, steps),colour="#000099")
plot(activityWithOutNAs[['steps']], activityWithOutNAs[['date']])
plot(activityWithOutNAs[['interval']], activityWithOutNAs[['steps']], type='l')
library(ggplot2)
activity <- read.csv("data/activity.csv")
dir()
library("plyr", lib.loc="/Library/Frameworks/R.framework/Versions/3.1/Resources/library")
library(ggplot2)
library(plyr)
# Data is loaded into a data frame called activity
activity <- read.csv("data/activity.csv")
# The NAs values are removed from activity
activityWithOutNAs <- na.omit(activity)
library(ggplot2)
library(plyr)
# Data is loaded into a data frame called activity
activity <- read.csv("data/activity.csv")
# The NAs values are removed from activity
activityWithOutNAs <- na.omit(activity)
activity <- read.csv("data/activity.csv")
dir()
setwd(/Users/artasom/VCS/github/RepData_PeerAssessment1)
dir("/Users/artasom/VCS/github/RepData_PeerAssessment1")
# Loading libraries
library(ggplot2)
library(plyr)
# Data is loaded into a data frame called activity
activity <- read.csv("data/activity.csv")
# The NAs values are removed from activity
activityWithOutNAs <- na.omit(activity)
dir()
setwd('/Users/artasom/VCS/github/RepData_PeerAssessment1')
dir()
activity <- read.csv("data/activity.csv")
# Loading libraries
library(ggplot2)
library(plyr)
# Data is loaded into a data frame called activity
activity <- read.csv("data/activity.csv")
# The NAs values are removed from activity
activityWithOutNAs <- na.omit(activity)
#the histogram is made on the processed data (activity records without NAs), and grouped by 'date'
hist(rowsum(activityWithOutNAs[['steps']], activityWithOutNAs[['date']]),
breaks=20,col = "grey", main = 'Total number of steps taken each day')
mean(activityWithOutNAs[['steps']])
median(activityWithOutNAs[['steps']])
```{r echo=TRUE}
# First of all, we calculate the steps of each interval, averaged across all days
activityWONAsMeanStepsByDate <- ddply(activityWithOutNAs, "interval", summarise, mean.steps = mean(steps))
# And then, they are plot
ggplot(activityWONAsMeanStepsByDate) + geom_line(aes(interval, mean.steps),type = "l",colour="#000099")
# 5-minute interval that contains the maximum number of steps
activityWONAsMeanStepsByDate[which.max(activityWONAsMeanStepsByDate[,'mean.steps']),]
nrow(activity)-sum(complete.cases(activity))
View(activity)
View(activityWONAsMeanStepsByDate)
for (i in 1:nrow(activity))  {
if (is.na(activity[i,'steps']))
activity[i,'steps'] <- activityWONAsMeanStepsByDate$mean.steps[activityWONAsMeanStepsByDate$interval==activity[i,'interval'],]
}
for (i in 1:nrow(activity))  {
if (is.na(activity[i,'steps']))
activity[i,'steps'] <- activityWONAsMeanStepsByDate$mean.steps[activityWONAsMeanStepsByDate$interval==activity[i,'interval']]
}
View(activity)
activity[activity$interval==0]
activity[,activity$interval==0]
activity[activity$interval==0,]
View(activityWithOutNAs)
weekdays((activity[1,'date']))
weekdays()
activity[1,'date']
weekdays(activity[1,'date'])
weekdays('2012-10-01')
weekdays(2012-10-01)
activity$date[1]
as.Date(activity$date[1])
weekdays(as.Date(activity$date[1]))
weekdays(as.Date(activity$date[1]),%W)
weekdays(as.Date(activity$date[1]))
?weekdays
as.numeric(weekdays(as.Date(activity$date[1])))
weekdays("2012-10-01")
weekdays(as.Date(activity$date[1]))
weekdays(as.Date(activity$date[2]))
weekdays(as.Date(activity$date[200]))
weekdays(as.Date(activity$date[500]))
weekdays(as.Date(activity$date[1]))+weekdays(as.Date(activity$date[500]))
weekdays(as.Date(activity$date[1]))<weekdays(as.Date(activity$date[500]))
weekdays(as.Date(activity$date[1]))<"Tuesday"
weekdays(as.Date(activity$date[1]))<"Monday"
if (weekdays(activity[i,'date'])<'Saturday')
activity[i,'typeOfDay'] <- "weekday"
else
activity[i,'typeOfDay'] <- "weekend"
View(activity)
View(activity)
for (i in 1:nrow(activity))  {
activity$typeOfDay[i] <- ifelse(weekdays(as.Date(activity[i,'date']))<"Saturday",weekday, weekend)
}
ifelse(weekdays(as.Date(activity[1,'date']))<"Saturday","weekday","weekend")
activity$typeOfDay[1] <- ifelse(weekdays(as.Date(activity[1,'date']))<"Saturday","weekday","weekend")
ifelse(weekdays(as.Date(activity[1,'date']))<"Saturday","weekday","weekend")
ifelse(weekdays(as.Date(activity[500,'date']))<"Saturday","weekday","weekend")
activity$typeOfDay[1] <- "weekend"
View(activity)
activity$typeOfDay[1] <- ifelse(weekdays(as.Date(activity[1,'date']))<"Saturday","weekday","weekend")
View(activity)
View(activity)
View(activity)
# Reproducible Research: Peer Assessment 1
## Loading and preprocessing the data
```{r echo=TRUE}
# Loading libraries
library(ggplot2)
library(plyr)
# Data is loaded into a data frame called activity
activity <- read.csv("data/activity.csv")
# The NAs values are removed from activity
activityWithOutNAs <- na.omit(activity)
```
## What is mean total number of steps taken per day?
```{r echo=TRUE}
#the histogram is made on the processed data (activity records without NAs), and grouped by 'date'
hist(rowsum(activityWithOutNAs[,'steps'], activityWithOutNAs[,'date']),
breaks=20,col = "grey", main = 'Total number of steps taken each day (activity with out NAs')
mean(activityWithOutNAs[,'steps'])
median(activityWithOutNAs[,'steps'])
```
## What is the average daily activity pattern?
```{r echo=TRUE}
# First of all, we calculate the steps of each interval, averaged across all days
activityWONAsMeanStepsByDate <- ddply(activityWithOutNAs, "interval", summarise, mean.steps = mean(steps))
# And then, they are plot
ggplot(activityWONAsMeanStepsByDate) + geom_line(aes(interval, mean.steps),type = "l",colour="#000099")
# 5-minute interval that contains the maximum number of steps
activityWONAsMeanStepsByDate[which.max(activityWONAsMeanStepsByDate[,'mean.steps']),]
```
## Imputing missing values
```{r echo=TRUE}
# Number of rows with NAs
nrow(activity)-sum(complete.cases(activity))
# Strategy for filling the NA values: the mean for that 5-minute interval, calculated before
# Iteration over activity
for (i in 1:nrow(activity))  {
# if the step value of the row is NA
if (is.na(activity[i,'steps']))
# the value 'mean.steps' from activityWONAsMeanStepsByDate that matches the intervals of both data frames is taken
activity[i,'steps'] <- activityWONAsMeanStepsByDate$mean.steps[activityWONAsMeanStepsByDate$interval==activity[i,'interval']]
}
#the histogram is made on the original data which NA values had been filled
hist(rowsum(activity[,'steps'], activity[,'date']),
breaks=20,col = "grey", main = 'Total number of steps taken each day (activity with NAs filles')
mean(activity[,'steps'])
median(activity[,'steps'])
# The histogram has the same shape as the previous one. But the frecuencies are higher. This is becasue we increased the number of steps but with values that were averaged.
```
## Are there differences in activity patterns between weekdays and weekends?
```{r}
#First of all, we create a new factor variable in the by iterating over activity
for (i in 1:nrow(activity))  {
activity$typeOfDay[i] <- ifelse(weekdays(as.Date(activity[i,'date']))<"Saturday","weekday","weekend")
}
```
activity
View(activity)
#First of all, we create a new factor variable in the by iterating over activity
for (i in 1:nrow(activity))  {
activity$typeOfDay[i] <- ifelse(weekdays(as.Date(activity$date[i]))<"Saturday","weekday","weekend")
}
```{r echo=TRUE}
# Then, we calculate the steps of each interval, averaged across weekdays/weekends
activityMeanStepsByTypeOfDay <- ddply(activity, .(interval, typeOfDay), summarise, mean.steps = mean(steps))
ggplot(activityMeanStepsByTypeOfDay, aes(interval=factor(round_any(interval,0.5)), y=mean.steps,fill=typeOfDay))+
geom_boxplot()+
facet_grid(.~typeOfDay)+
labs(interval="interval"))
ggplot(activityMeanStepsByTypeOfDay, aes(interval=factor(round_any(interval,0.5)), y=mean.steps,fill=typeOfDay))+
geom_boxplot()+
facet_grid(.~typeOfDay)+
labs(interval="interval")
ggplot(activityMeanStepsByTypeOfDay, aes(interval=factor(round_any(interval,0.5)), y=mean.steps,fill=typeOfDay))
ggplot(activityMeanStepsByTypeOfDay) + geom_line(aes(interval, mean.steps),type = "l",colour="#000099",y=mean.steps,fill=typeOfDay)
ggplot(activityMeanStepsByTypeOfDay) + geom_line(aes(interval, mean.steps),type = "l",colour="#000099",y='mean.steps',fill='typeOfDay')
xyplot(interval ~ mean.steps | typeOfDay, data=activityMeanStepsByTypeOfDay, pch=".",layout=c(1, 3), aspect=1, index.cond=list(3:1))
library("lattice", lib.loc="/Library/Frameworks/R.framework/Versions/3.1/Resources/library")
xyplot(interval ~ mean.steps | typeOfDay, data=activityMeanStepsByTypeOfDay, pch=".",layout=c(1, 3), aspect=1, index.cond=list(3:1))
xyplot(mean.steps ~ interval | typeOfDay, data=activityMeanStepsByTypeOfDay, pch=".",layout=c(1, 3), aspect=1, index.cond=list(3:1))
View(activityMeanStepsByTypeOfDay)
xyplot(mean.steps ~ interval | typeOfDay, data=activityMeanStepsByTypeOfDay, pch=".",type="l", aspect=1)
hist(rowsum(activityWithOutNAs[,'steps'], activityWithOutNAs[,'date']),
breaks=50,col = "grey", main = 'Total number of steps taken each day (activity with out NAs')
hist(rowsum(activity[,'steps'], activity[,'date']),
breaks=50,col = "grey", main = 'Total number of steps taken each day (activity with NAs filles')
hist(rowsum(activityWithOutNAs[,'steps'], activityWithOutNAs[,'date']),
breaks=50,col = "grey", main = 'Total number of steps taken each day (activity with out NAs')
hist(rowsum(activity[,'steps'], activity[,'date']),
breaks=50,col = "grey", main = 'Total number of steps taken each day (activity with NAs filles')
View(activityWithOutNAs)
View(activityWithOutNAs)
View(activity)
hist(rowsum(activityWithOutNAs[,'steps'], activityWithOutNAs[,'interval']),
breaks=50,col = "grey", main = 'Total number of steps taken each day (activity with out NAs')
